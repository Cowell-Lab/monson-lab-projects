{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b59ce807",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18d833ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f44548",
   "metadata": {},
   "source": [
    "# Data\n",
    "Read and file `NaN` values with `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff22dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv('./data/MyExpt_Image(in).csv', na_values='nan',).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7115837",
   "metadata": {},
   "source": [
    "# Create compressed data\n",
    "Sort values by `Series_G3BP` and `Metadata_T` and extract columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66ad5cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_cols = ['Series_G3BP', 'Metadata_T', 'Count_G3BP_Puncta', 'Count_Nuclei', 'Mean_G3BP_Puncta_AreaShape_Area', 'Mean_G3BP_Puncta_Intensity_MeanIntensity_G3BP', 'Mean_Nuclei_AreaShape_Area']\n",
    "compressed = raw.sort_values(['Series_G3BP', 'Metadata_T']).loc[:,extract_cols]\n",
    "\n",
    "compressed['Total_SG_Area'] = compressed['Count_G3BP_Puncta'] * compressed['Mean_G3BP_Puncta_AreaShape_Area']\n",
    "compressed['Total_SG_Signal'] = compressed['Total_SG_Area'] * compressed['Mean_G3BP_Puncta_Intensity_MeanIntensity_G3BP']\n",
    "compressed['Total_Nuc_Area'] = compressed['Mean_Nuclei_AreaShape_Area'] * compressed['Count_Nuclei']\n",
    "compressed.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ea671",
   "metadata": {},
   "source": [
    "# Create sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a0bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(compressed, column_labels, series_starts, continuous_index=0, s_idx=0):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    compressed : pd.DataFrame()\n",
    "        - a pandas DataFrame with the columns extracted from original data\n",
    "    column_labels : list\n",
    "        - a list of columns labels to extract from compressed\n",
    "    series_starts : list\n",
    "        - a list of start indexes for new series within the data for the Series_G3BP column\n",
    "    continous_index : int\n",
    "        - used for tracking with compressed\n",
    "    s_idx : int\n",
    "        - used for tracking index of series_starts\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame()\n",
    "        - a pandas DataFrame with normalized data from compressed\n",
    "    continous_index : int\n",
    "        - used for tracking with compressed; returned int for next group\n",
    "    s_idx : int\n",
    "        - used for tracking index of series_starts; returned into for next group\n",
    "    '''\n",
    "    df = pd.DataFrame(columns=column_labels)\n",
    "    # go through all raw\n",
    "    for i in range(continuous_index, len(compressed)):\n",
    "        # see if Series_G3BP is equal to or greater than series_start\n",
    "        if compressed.loc[i,'Series_G3BP'] >= series_starts[s_idx]:\n",
    "            # see if we can check next series_start\n",
    "            if s_idx <= len(series_starts)-1:\n",
    "                # check next series_start\n",
    "                if compressed.loc[i,'Series_G3BP'] < (series_starts[s_idx]+(series_starts[1]-series_starts[0])): # would prefer to have < series_start[s_idx+1] for the edge case that there are not n0-n9 in the final group, or if there aren't 10 groups in the future\n",
    "                    # construct dataframe\n",
    "                    df.loc[i-continuous_index, 'Series_G3BP'] = compressed.loc[i, 'Series_G3BP']\n",
    "                    df.loc[i-continuous_index, 'Metadata_T'] = compressed.loc[i, 'Metadata_T']\n",
    "                    df.loc[i-continuous_index, 'Count_G3BP_Puncta'] = compressed.loc[i, 'Count_G3BP_Puncta']\n",
    "                    df.loc[i-continuous_index, 'Count_Nuclei'] = compressed.loc[i, 'Count_Nuclei']\n",
    "                    df.loc[i-continuous_index, 'Total_SG_Area'] = compressed.loc[i, 'Total_SG_Area']\n",
    "                    df.loc[i-continuous_index, 'Total_SG_Signal'] = compressed.loc[i, 'Total_SG_Signal']\n",
    "                    df.loc[i-continuous_index, 'Total_Nuc_Area'] = compressed.loc[i, 'Total_Nuc_Area']\n",
    "                else:\n",
    "                    continuous_index = i\n",
    "                    s_idx += 1\n",
    "                    break\n",
    "            else: \n",
    "                print('ERROR: s_idx > len(series_starts) @ '+str(i)+' with s_idx='+str(s_idx))\n",
    "                break\n",
    "        else:\n",
    "            print('ERROR: Series_G3BP below series_start')\n",
    "            break\n",
    "    else:\n",
    "        continuous_index=i+1\n",
    "    \n",
    "    # norm by max\n",
    "    max_by_series = df.groupby('Series_G3BP').max()\n",
    "    for i in range(len(df)):\n",
    "        i_max = df.loc[i,'Series_G3BP']\n",
    "        df.loc[i, 'Count_G3BP_Puncta_norm_max'] = df.loc[i, 'Count_G3BP_Puncta']*100/max_by_series.loc[i_max,'Count_G3BP_Puncta']\n",
    "        df.loc[i, 'Count_Nuclei_norm_max'] = df.loc[i, 'Count_Nuclei']*100/max_by_series.loc[i_max,'Count_Nuclei']\n",
    "        df.loc[i, 'Total_SG_Area_norm_max'] = df.loc[i, 'Total_SG_Area']*100/max_by_series.loc[i_max,'Total_SG_Area']\n",
    "        df.loc[i, 'Total_SG_Signal_norm_max'] = df.loc[i, 'Total_SG_Signal']*100/max_by_series.loc[i_max,'Total_SG_Signal']\n",
    "        df.loc[i, 'Total_Nuc_Area_norm_max'] = df.loc[i, 'Total_Nuc_Area']*100/max_by_series.loc[i_max,'Total_Nuc_Area']\n",
    "\n",
    "    # norm by nuclei count\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i, 'Count_G3BP_Puncta_norm_count_nuclei'] = df.loc[i, 'Count_G3BP_Puncta']/compressed.loc[continuous_index-len(df)+i,'Count_Nuclei']\n",
    "        df.loc[i, 'Count_Nuclei_norm_count_nuclei'] = df.loc[i, 'Count_Nuclei']/compressed.loc[continuous_index-len(df)+i,'Count_Nuclei']\n",
    "        df.loc[i, 'Total_SG_Area_norm_count_nuclei'] = df.loc[i, 'Total_SG_Area']/compressed.loc[continuous_index-len(df)+i,'Count_Nuclei']\n",
    "        df.loc[i, 'Total_SG_Signal_norm_count_nuclei'] = df.loc[i, 'Total_SG_Signal']/compressed.loc[continuous_index-len(df)+i,'Count_Nuclei']\n",
    "        df.loc[i, 'Total_Nuc_Area_norm_count_nuclei'] = df.loc[i, 'Total_Nuc_Area']/compressed.loc[continuous_index-len(df)+i,'Count_Nuclei']\n",
    "\n",
    "    # norm by nuclear area\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i, 'Count_G3BP_Puncta_norm_nuc_area'] = df.loc[i, 'Count_G3BP_Puncta']*100/compressed.loc[continuous_index-len(df)+i,'Total_Nuc_Area']\n",
    "        df.loc[i, 'Count_Nuclei_norm_nuc_area'] = df.loc[i, 'Count_Nuclei']*100/compressed.loc[continuous_index-len(df)+i,'Total_Nuc_Area']\n",
    "        df.loc[i, 'Total_SG_Area_norm_nuc_area'] = df.loc[i, 'Total_SG_Area']*100/compressed.loc[continuous_index-len(df)+i,'Total_Nuc_Area']\n",
    "        df.loc[i, 'Total_SG_Signal_norm_nuc_area'] = df.loc[i, 'Total_SG_Signal']*100/compressed.loc[continuous_index-len(df)+i,'Total_Nuc_Area']\n",
    "        df.loc[i, 'Total_Nuc_Area_norm_nuc_area'] = df.loc[i, 'Total_Nuc_Area']*100/compressed.loc[continuous_index-len(df)+i,'Total_Nuc_Area']\n",
    "\n",
    "    return df, continuous_index, s_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4792312",
   "metadata": {},
   "source": [
    "# Run all labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bbf0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting Data\n",
      "Writing Group: JRR_UT\n",
      "Writing Group: TGM_UT\n",
      "Writing Group: JRR_500\n",
      "Writing Group: TGM_500\n",
      "Writing Group: JRR_250\n",
      "Writing Group: TGM_250\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Gives the start value for each group in `Series_G3BP` that is used to create groups `JRR_UT`, `TGM_UT`, `JRR_250`, `TGM_250`, `JRR_500`, `TGM_500`. \n",
    "# Assumes consecutive values to make group (i.e. 0-9 = `JRR_UT`)\n",
    "series_starts = [0, 10, 20, 30, 40, 50]\n",
    "# group names corresponding to index starts\n",
    "groups = ['JRR_UT', 'TGM_UT', 'JRR_500', 'TGM_500', 'JRR_250', 'TGM_250']\n",
    "# columns used in the final spreadsheet\n",
    "make_cols = ['Series_G3BP', 'Metadata_T', 'Count_G3BP_Puncta', 'Count_Nuclei', 'Total_SG_Area',  'Total_SG_Signal', 'Total_Nuc_Area'] # columns made each time\n",
    "# File naming scheme\n",
    "file_prefix = '20250521_Video_Analysis_'\n",
    "file_suffix = '_nomalized_test'\n",
    "excel_file_name = file_prefix+file_suffix[1:]+'.xlsx'\n",
    "\n",
    "# run\n",
    "# delete existing data so we can append df as new sheet, otherwise new xlsx files are created each time\n",
    "if os.path.exists('./output/'+excel_file_name):\n",
    "    print('Deleting Data')\n",
    "    os.remove('./output/'+excel_file_name)\n",
    "\n",
    "# write data\n",
    "continuous_index = 0\n",
    "s_idx = 0\n",
    "for group in groups:\n",
    "    print('Writing Group: '+group)\n",
    "    df, continuous_index, s_idx = make_df(compressed, make_cols, series_starts, continuous_index, s_idx)\n",
    "    # df.to_csv('./output/'+file_prefix+group+file_suffix+'.csv', index=False)\n",
    "\n",
    "    if os.path.exists('./output/'+excel_file_name):\n",
    "        # this engine allows for appending to an existing xlsx file (but can't create one)\n",
    "        with pd.ExcelWriter('./output/'+excel_file_name, mode='a', engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name=group, index=False)\n",
    "    else:\n",
    "        # this engine can create and then write to the xlsx file (but can't append to one)\n",
    "        with pd.ExcelWriter('./output/'+excel_file_name, mode='w', engine='xlsxwriter') as writer:\n",
    "            df.to_excel(writer, sheet_name=group, index=False)\n",
    "\n",
    "print('Done')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2040ae33",
   "metadata": {},
   "source": [
    "# Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b310a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
